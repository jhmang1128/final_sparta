{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " path ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "\n",
      "ROOT_DIR : Z:\\maeng_dir\\sparta_camp\\final_sparta\n",
      "pwd : z:\\maeng_dir\\sparta_camp\\final_sparta\\ipython\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "print(\"\\n\\n path ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\\n\")\n",
    "load_dotenv()\n",
    "ROOT_DIR = os.getenv(\"ROOT_DIR\")\n",
    "poppler_path = os.path.join(ROOT_DIR, r\"lib\\poppler-24.08.0\\Library\\bin\")\n",
    "\n",
    "\n",
    "print(\"ROOT_DIR :\", ROOT_DIR)\n",
    "print(\"pwd :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR - pdf to text\n",
    "https://sooeun67.github.io/data%20science/ocr-comparison/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "#### Data Load & Split\n",
    "print(\"\\n\\n Data Load & Split ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\\n\")\n",
    "whole_data = CSVLoader(file_path=\"./data/test.csv\", encoding='utf-8').load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "split_data = splitter.split_documents(whole_data)\n",
    "\n",
    "\n",
    "print(type(split_data[0]))\n",
    "print(split_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.parsers import RapidOCRBlobParser\n",
    "\n",
    "\n",
    "#### path\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "pdf_image_path_05 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_05.pdf\")\n",
    "pdf_image_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"soil_suitability.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Load & Split\n",
    "print(\"\\n\\n Data Load & Split ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\\n\")\n",
    "# page_list = PyPDFLoader(file_path = pdf_image_path_01,).load()\n",
    "# page_list = PyPDFLoader(file_path = pdf_image_path_01, mode=\"page\").load()\n",
    "# page_list = PyPDFLoader(file_path = pdf_image_path_01, extract_images=True).load()\n",
    "\n",
    "page_list = PyPDFLoader(\n",
    "            file_path = pdf_image_path_01,\n",
    "            mode=\"page\",\n",
    "            images_inner_format=\"markdown-img\",\n",
    "            images_parser=RapidOCRBlobParser(),\n",
    "            ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page_list[0])\n",
    "# print(page_list[0])\n",
    "# print(page_list[1])\n",
    "# print(page_list[1].page_content[:300])\n",
    "# print(page_list[2])\n",
    "# print(page_list[2].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "#### path\n",
    "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\USER\\AppData\\Local\\Tesseract-OCR\\tesseract.exe\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "pdf_image_path_05 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_05.pdf\")\n",
    "pdf_image_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"soil_suitability.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " page 0 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "AA\n",
      "\n",
      "melonggena 7.\n",
      "\n",
      "생육에 따른 기상득성(0)\n",
      "\n",
      "sores                   a\n",
      "\n",
      "직물 재배에 FLAP 화혁싱\n",
      "\n",
      "re |   소젠        —\n",
      "ais | BES] 거소    !      Me | PS\n",
      "ase                     Erle                      ure\n",
      "Tian Dai Pan\n",
      "\n",
      "\n",
      " page 1 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "생육에 따른 기상두성(0)\n",
      "Ed          1    [    STG\n",
      "\n",
      "To 515. 1\n",
      "\n",
      "Amel 알맞은 도양 ada 득성 및 Sela)\n",
      "\n",
      "1        ae        두실 | ae Foo\n",
      "z       100\n",
      "\n",
      "직물 Add 알맛은 Ve Bad\n",
      "\n",
      "ml         Feccations\n",
      "\n",
      "ow |        1s Shel Becirs wt\n",
      "\n",
      "a                    My\n",
      "\n",
      "ele         더                   마의\n",
      "\n",
      "ma | ena          Wea | 1815\n",
      "\n",
      "=~\n",
      "\n",
      " page 2 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "생육에 따는 기상두\n",
      "\n",
      "Toe\n",
      "\n",
      "Zee\n",
      "\n",
      "Ae 제배에 알맞은 노\n",
      "\n",
      "기명    경사!\n",
      "\n",
      "두신\n",
      "\n",
      "당일 ae\n",
      "\n",
      "ee eo\n",
      "\n",
      "재매에 알맞은 도양 sa) 4)\n",
      "re            nella            Feccations\n",
      "은                           clea\n",
      "ase | OS                      Me\n",
      "lint                        wa\n",
      "= am                  0907\n",
      "\n",
      "Jatt FARR 가기\n",
      "\n",
      "asl\n",
      "\n",
      "goo}, ola)\n",
      "\n",
      " page 3 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "=\n",
      "\n",
      "생육에 따른 기상두성(\n",
      "\n",
      "a\n",
      "\n",
      "TEE\n",
      "\n",
      "Saale       a\n",
      "\n",
      "가을 제때에 TSP ey 형데식 득싱 및 을리성\n",
      "7        ae           za       fata\n",
      "Ee       ;     aaa\n",
      "\n",
      "가을 제매에 RP 노양 회학성\n",
      "\n",
      "-         ee,\n",
      "ve          ar      VRE OLS\n",
      "\n",
      "사도\n",
      "\n",
      "-28-\n",
      "\n",
      " page 4 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "5) ase\n",
      "\n",
      "us midanvis nar f\n",
      "\n",
      "생육에 따른 기상득성(0)\n",
      "\n",
      "Dee\n",
      "\n",
      "Taye\n",
      "\n",
      "ae 제배에 안낮은 노암 혐데석 두성\n",
      "\n",
      "|        4\n",
      "\n",
      "mal\n",
      "\n",
      "Ae 제배에 PEP ET 회학성\n",
      "\n",
      "eR,\n",
      "\n",
      "매 | ow | UE\n",
      "\n",
      "4\n",
      "\n",
      "ala\n",
      "\n",
      "a\n",
      "\n",
      "\n",
      " page 5 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "=A\n",
      "\n",
      "재매에 알맞은 도양 Sha] 4)\n",
      "1,         Feccations\n",
      "와\n",
      "\n",
      "미 | ow\n",
      "Bs\n",
      "\n",
      "사연\n",
      "Ca         Mi\n",
      "ali\n",
      "\n",
      "0020\n",
      "\n",
      "ㅠㅠ\n",
      "\n",
      "-3-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### OCR\n",
    "with pdfplumber.open(pdf_image_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        page_image = page.to_image()\n",
    "        full_page_image = page_image.original.convert(\"RGB\") # PIL 이미지로 변환\n",
    "\n",
    "        ocr_text = pytesseract.image_to_string(full_page_image, lang=\"kor+eng\")\n",
    "        \n",
    "        print(f\" page {i} ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
    "        print(ocr_text)\n",
    "        \n",
    "        if i == 5:\n",
    "            break\n",
    "        \n",
    "        # page_list[i] = Document(page_content=ocr_text, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pypdfium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pypdfium2\n",
    "import pypdfium2 as pdfium\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "pdf_image_path_05 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_05.pdf\")\n",
    "pdf_image_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"soil_suitability.pdf\")\n",
    "\n",
    "\n",
    "pdf = pdfium.PdfDocument(pdf_image_path_01)\n",
    "\n",
    "version = pdf.get_version()  # get the PDF standard version\n",
    "n_pages = len(pdf)  # get the number of pages in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdf)):\n",
    "    page = pdf[i]  \n",
    "    text = page.get_textpage().get_text_range()\n",
    "    # text = page.get_textpage().get_text_bounded()\n",
    "    \n",
    "    print(\"text :\", text)\n",
    "    # break\n",
    "    # extracted_text.append(text)\n",
    "\n",
    "# \"\\n\".join(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ghostscript - pdf to pdf (with text)\n",
    "https://ghostscript.com/blog/ocr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "gs_path = r\"C:\\Program Files\\gs\\gs10.04.0\\bin\\gswin64c.exe\"\n",
    "os.environ[\"TESSDATA_PREFIX\"] = r\"C:\\Program Files\\Tesseract-OCR\\tessdata\"\n",
    "\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "\n",
    "output_pdf_path = os.path.join(ROOT_DIR, \"data\", \"output.PDF\")\n",
    "output_txt_path = os.path.join(ROOT_DIR, \"data\", \"ocr_text.txt\")\n",
    "output_json_path = os.path.join(ROOT_DIR, \"data\", \"ocr_text.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ocr\n",
    "def ocr_with_ghostscript(input_pdf, output_pdf, lang=\"kor\"):\n",
    "    gs_command = [\n",
    "        gs_path,\n",
    "        # \"gs\",\n",
    "        \"-dNOPAUSE\",\n",
    "        \"-dBATCH\",\n",
    "        \"-sDEVICE=ocr\",\n",
    "        f\"-sOCRLanguage={lang}\",  # 한글 OCR 설정\n",
    "        \"-r300\",\n",
    "        f\"-sOutputFile={output_pdf}\",\n",
    "        input_pdf\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(gs_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"OCR 적용 완료: {output_pdf}\")\n",
    "    else:\n",
    "        print(f\"오류 발생: {result.stderr.decode()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_with_ghostscript(pdf_image_path_01, output_pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aspose\n",
    "https://docs.aspose.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'aspose.ocr.RecognitionSettings' object has no attribute 'auto_denoising'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 인식 설정 초기화\u001b[39;00m\n\u001b[0;32m      8\u001b[0m settings \u001b[38;5;241m=\u001b[39m ocr\u001b[38;5;241m.\u001b[39mRecognitionSettings()\n\u001b[1;32m----> 9\u001b[0m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_denoising\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m settings\u001b[38;5;241m.\u001b[39mauto_contrast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 인식 배치에 파일 추가\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'aspose.ocr.RecognitionSettings' object has no attribute 'auto_denoising'"
     ]
    }
   ],
   "source": [
    "# 이 코드 예제는 Python에서 스캔한 PDF 문서에서 텍스트를 인식하고 추출하는 방법을 보여줍니다.\n",
    "import aspose.ocr as ocr\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "\n",
    "\n",
    "# OCR 엔진 초기화\n",
    "api = ocr.AsposeOcr()\n",
    "\n",
    "# 인식 설정 초기화\n",
    "settings = ocr.RecognitionSettings()\n",
    "settings.auto_denoising = True\n",
    "settings.auto_contrast = True\n",
    "\n",
    "# 인식 배치에 파일 추가\n",
    "files = ocr.OcrInput(ocr.InputType.PDF)\n",
    "\n",
    "# 스캔한 PDF에 액세스하여 페이지 번호와 총 페이지 수를 설정하세요.\n",
    "files.add(pdf_image_path_01, 0, 1)\n",
    "\n",
    "# 텍스트를 인식\n",
    "result = api.recognize(files , settings)\n",
    "\n",
    "# 인식 결과 인쇄\n",
    "print(result[0].recognition_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pdf to image pdf2imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pdf2image\n",
    "import pdf2image\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "poppler_path = os.path.join(ROOT_DIR, r\"lib\\poppler-24.08.0\\Library\\bin\")\n",
    "\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### convert\n",
    "pages = convert_from_path(pdf_image_path, poppler_path=poppler_path)\n",
    "\n",
    "for count, page in enumerate(pages):\n",
    "    print(count)\n",
    "    # break\n",
    "    # page.save(f'out{count}.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytesseract (with image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "#### lib path\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = os.path.join(ROOT_DIR, r\"lib\\poppler-24.08.0\\Library\\bin\")\n",
    "\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### convert\n",
    "images = convert_from_path(\n",
    "                            pdf_image_path_01,\n",
    "                            dpi = 300,\n",
    "                            poppler_path=poppler_path,\n",
    "                        )\n",
    "\n",
    "f = open(\"./ocr_text.txt\", \"w\")\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    ocr_text = pytesseract.image_to_string(image, lang=\"kor+eng\")\n",
    "    f.write(f\"page {i}\")\n",
    "    f.write(ocr_text)\n",
    "    \n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naver clova\n",
    "pdf to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "#### load keys\n",
    "load_dotenv()\n",
    "api_url = os.getenv(\"invoke_url\")\n",
    "secret_key = os.getenv(\"secret_key\")\n",
    "\n",
    "\n",
    "#### path\n",
    "# pdf_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "pdf_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_05.pdf\")\n",
    "\n",
    "# print(api_url)\n",
    "# print(secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### api params\n",
    "request_json = {\n",
    "    'images': [\n",
    "        {\n",
    "            'format': 'pdf',\n",
    "            'name': 'demo'\n",
    "        }\n",
    "    ],\n",
    "    'requestId': str(uuid.uuid4()),\n",
    "    'version': 'V2',\n",
    "    'timestamp': int(round(time.time() * 1000))\n",
    "}\n",
    "\n",
    "payload = {'message': json.dumps(request_json).encode('UTF-8')}\n",
    "files = [\n",
    "    ('file', open(pdf_path,'rb'))\n",
    "]\n",
    "headers = {\n",
    "  'X-OCR-SECRET': secret_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#### api\n",
    "response = requests.request(\"POST\", api_url, headers=headers, data = payload, files = files)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### to text (test)\n",
    "pages = response.json()['images']\n",
    "\n",
    "fw = open(\"../test_ocr_naver.txt\", \"w\")\n",
    "for page in pages:\n",
    "    for i_obj, obj in enumerate(page['fields']):\n",
    "        text = obj['inferText']\n",
    "        print(obj, file = fw)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#### save raw result\n",
    "print(type(response.json()))\n",
    "\n",
    "with open(\"../raw_reponse_naver_ocr.json\", \"w\") as f:\n",
    "    f.write(json.dumps(response.json(), ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "#### load raw response\n",
    "# pages = response.json()['images'] # from memory\n",
    "with open(\"../raw_reponse_naver_ocr.json\", \"r\") as f: # from .json\n",
    "    response_json = json.load(f)\n",
    "    pages = response_json['images']    \n",
    "\n",
    "\n",
    "#### page to dict\n",
    "crop_dict = {}\n",
    "for page in pages:\n",
    "    crop_name = \"\"\n",
    "    full_text = \"\"\n",
    "    \n",
    "    \n",
    "    #### obejct to line\n",
    "    for i_obj, obj in enumerate(page['fields']):\n",
    "        text = obj['inferText'].replace(\" \", \"\")\n",
    "        \n",
    "        if i_obj == 0: # number\n",
    "            continue\n",
    "        elif i_obj == 1: # crop name\n",
    "            crop_name = text\n",
    "            continue\n",
    "        elif \"- \" in text and \" -\" in text:\n",
    "            break\n",
    "        else:\n",
    "            full_text += text + \" \"\n",
    "            \n",
    "        if obj['lineBreak']:\n",
    "            full_text += \"\\n\"\n",
    "    \n",
    "    crop_dict[crop_name] = full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = \"\"\"\n",
    "    식물명\n",
    "    \n",
    "    학명(정명)\n",
    "    학명 (정명)\n",
    "    \n",
    "    분류\n",
    "    \n",
    "    지역별다른이름\n",
    "    지역별 다른이름\n",
    "    \n",
    "    작물의생태\n",
    "    작물의 생태\n",
    "    \n",
    "    작물생육에따른\n",
    "    작물생육에 따른\n",
    "    작물 생육에따른\n",
    "    작물 생육에 따른\n",
    "    \n",
    "    작물재배에알맞은\n",
    "    작물 재배에알맞은\n",
    "    작물재배에 알맞은\n",
    "    작물 재배에 알맞은\n",
    "\"\"\".replace(\"  \", \"\").split(\"\\n\")\n",
    "\n",
    "col_list = list(filter(None, col_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### parsing inner\n",
    "for crop_name in crop_dict.keys():\n",
    "    #### line\n",
    "    line_list = crop_dict[crop_name].split(\"\\n\")\n",
    "    line_list = list(filter(None, line_list))\n",
    "    idx_list = []\n",
    "    \n",
    "    \n",
    "    #### extract col idx\n",
    "    for idx_l, line in enumerate(line_list):\n",
    "        for col_name in col_list:\n",
    "            if col_name in line:\n",
    "                idx_list.append(idx_l)\n",
    "    \n",
    "    \n",
    "    #### check col len\n",
    "    if not len(idx_list) == 8:\n",
    "        print(\"col len error :\", crop_name)\n",
    "        print(len(idx_list), idx_list)\n",
    "        for idx in idx_list:\n",
    "            print(line_list[idx])\n",
    "        print(line_list)\n",
    "        break\n",
    "    \n",
    "    \n",
    "    #### process\n",
    "    inner_dict = {}\n",
    "    lines_text = \"\"\n",
    "    \n",
    "    \n",
    "    for idx_l, line in enumerate(line_list):\n",
    "        if idx_l in idx_list[0:3]:\n",
    "            idx_cut = line.find(\" \")\n",
    "            inner_dict[line[:idx_cut]] = line[idx_cut:].strip()\n",
    "            \n",
    "        elif idx_l in idx_list[3:4]:\n",
    "            idx_cut = line.find(\"다른이름\")+4\n",
    "            inner_dict[line[:idx_cut].replace(\" \", \"\")] = line[idx_cut:].strip()\n",
    "            \n",
    "        elif idx_l in idx_list[4:]:\n",
    "            col_name = line.replace(\" \", \"\").replace(\"(°C)\", \"\").replace(\"·\", \"\")\n",
    "            inner_dict[col_name] = \"\"\n",
    "        \n",
    "        else:\n",
    "            inner_dict[col_name] += line + \"\\n\"\n",
    "            \n",
    "            \n",
    "    crop_dict[crop_name] = inner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### parsing inner-inner (작물의생태생리특성)\n",
    "for crop_name in crop_dict.keys():\n",
    "    inner_inner_dict = {}\n",
    "    for val in crop_dict[crop_name][\"작물의생태생리특성\"][:-1].split(\"\\n\"):\n",
    "        if \"생태형\" in val: inner_inner_dict[\"생태형\"] = val[3:].strip()\n",
    "        elif \"월동여부\" in val:\n",
    "            val = val[4:].strip()\n",
    "            if val == \"O\" or val == \"o\" or val == \"○\": inner_inner_dict[\"월동여부\"] = True\n",
    "            elif val == \"X\" or val == \"x\": inner_inner_dict[\"월동여부\"] = False\n",
    "            else: print(\"error parssing 월동여부 value\", val)\n",
    "        elif \"생리특성\" in val: inner_inner_dict[\"생리특성\"] = val[4:].strip()\n",
    "        elif \"질소고정균\" in val:\n",
    "            val = val[10:].strip()\n",
    "            if val == \"O\" or val == \"o\" or val == \"○\": inner_inner_dict[\"질소고정균\"] = True\n",
    "            elif val == \"X\" or val == \"x\": inner_inner_dict[\"질소고정균\"] = False\n",
    "            else: print(\"error parssing 질소고정균 value\", type(val), val)\n",
    "        elif \"초본/목본\" in val: inner_inner_dict[\"초본/목본\"] = val[8:].strip()\n",
    "        else: \n",
    "            print(f\"error : parssing : 작물의생태생리특성 : {crop_name} :\")\n",
    "            print(\"error column\", val)\n",
    "            \n",
    "    crop_dict[crop_name][\"작물의생태생리특성\"] = inner_inner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop_name : 가지\n",
      "crop_name : 감나무\n",
      "crop_name : 감자\n",
      "crop_name : 감초\n",
      "\n",
      "crop_name : 강낭콩\n"
     ]
    }
   ],
   "source": [
    "#### parsing inner-inner ()\n",
    "for crop_name in crop_dict.keys():\n",
    "    ## 작물생육에따른기상특성\n",
    "    inner_inner_dict = {}\n",
    "    for line in crop_dict[crop_name][\"작물생육에따른기상특성\"][:-1].split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        vals_list = line.split(\" \")\n",
    "        if \"(\" in line and \")\" in line:\n",
    "            inner_inner_dict[line[:line.rfind(\" \")].strip()] = line[line.rfind(\" \"):].strip()\n",
    "        elif len(vals_list) == 2:\n",
    "            print()\n",
    "            inner_inner_dict[vals_list[0].strip()] = vals_list[1].strip()\n",
    "        elif len(vals_list) == 4:\n",
    "            inner_inner_dict[vals_list[0].strip()] = vals_list[1].strip()\n",
    "            inner_inner_dict[vals_list[2].strip()] = vals_list[3].strip()\n",
    "        else:\n",
    "            print(f\"error : parssing : 작물생육에따른기상특성 : {crop_name} :\")\n",
    "            print(\"line :\", line)\n",
    "            \n",
    "    crop_dict[crop_name][\"작물생육에따른기상특성\"] = inner_inner_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### parsing inner-inner ()\n",
    "for crop_name in crop_dict.keys():\n",
    "    # 작물재배에알맞은토양형태적특성및물리성\n",
    "    inner_inner_dict = {}\n",
    "    line_list = crop_dict[crop_name][\"작물재배에알맞은토양형태적특성및물리성\"][:-1].split(\"\\n\")\n",
    "    if len(line_list) == 2:\n",
    "        col_list = list(filter(None, line_list[0].split(\" \")))\n",
    "        val_list = list(filter(None, line_list[1].split(\" \")))\n",
    "        for idx_col, col_name in enumerate(col_list):\n",
    "            inner_inner_dict[col_name] = val_list[idx_col]\n",
    "            \n",
    "    elif len(line_list) == 4:\n",
    "        col_str = line_list[0].strip()+line_list[2]+line_list[1]\n",
    "        col_list = list(filter(None, col_str.split(\" \")))\n",
    "        val_list = list(filter(None, line_list[3].split(\" \")))\n",
    "        for idx_col, col_name in enumerate(col_list):\n",
    "            inner_inner_dict[col_name] = val_list[idx_col]\n",
    "    else:\n",
    "        print(f\"error : parssing : 작물재배에알맞은토양형태적특성및물리성 : {crop_name} : line num\")\n",
    "        print(line_list)\n",
    "    \n",
    "    crop_dict[crop_name][\"작물재배에알맞은토양형태적특성및물리성\"] = inner_inner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### cloumns\n",
    "col_list_dict = {\n",
    "                    \"7\" : \"산도 유기물 유효인산 칼륨 칼슘 마그네슘 양이온교환용량\".split(\" \"),\n",
    "                    \"8\" : \"산도 전기전도도 유기물 유효인산 칼륨 칼슘 마그네슘 양이온교환용량\".split(\" \"),\n",
    "                    \"9\" : \"산도 전기전도도 질산태질소 유기물 유효인산 칼륨 칼슘 마그네슘 양이온교환용량\".split(\" \"),\n",
    "                    \"10\" : \"산도 전기전도도 질산태질소 유기물 유효인산 칼륨 칼슘 마그네슘 양이온교환용량 붕소\".split(\" \"),\n",
    "            }\n",
    "    \n",
    "\n",
    "#### parsing inner-inner ()\n",
    "inner_inner_dict = {}\n",
    "for crop_name in crop_dict.keys():\n",
    "    ## 작물재배에알맞은토양화학성\n",
    "    inner_inner_dict = {}\n",
    "    line_list = crop_dict[crop_name][\"작물재배에알맞은토양화학성\"][:-1].split(\"\\n\")\n",
    "    for idx_line in range(1, len(line_list)+1):\n",
    "        line = line_list[-idx_line]\n",
    "        if line.count(\"-\") > 3:\n",
    "            val_list = list(filter(None, line.replace(\"-\", \"~\").split(\" \")))\n",
    "            for idx_val, val in enumerate(val_list):\n",
    "                if not \"~\" in val:\n",
    "                    val_list[idx_val] = \"<=\" + val\n",
    "            \n",
    "            try:\n",
    "                for idx_val, val in enumerate(val_list):\n",
    "                    inner_inner_dict[col_list_dict[f\"{len(val_list)}\"][idx_val]] = val\n",
    "            except:\n",
    "                print(f\"error : parssing : 작물재배에알맞은토양화학성 : {crop_name}\")\n",
    "                print(crop_name)\n",
    "                \n",
    "                \n",
    "    crop_dict[crop_name][\"작물재배에알맞은토양화학성\"] = inner_inner_dict\n",
    "    # print(crop_name)\n",
    "    # print(inner_inner_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save\n",
    "with open(\"../crop_soil.json\", \"w\") as f:\n",
    "    f.write(json.dumps(crop_dict, ensure_ascii=False, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_sparta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
